HyDFS: Hybrid Distributed File System – Initial Design Document

Scope

This document outlines the initial design of HyDFS, a distributed file system inspired by HDFS and Cassandra. The current version excludes fault tolerance and failure handling and focuses solely on core functionality under ideal conditions (no machine failures or network partitions). 

Before diving deeper we already have an code for Membership protocol which stores details of all the members. Whenever the HashSystem does a GetMembers it will get the whole list.

System Architecture

General terms:
1) coordinator/owner : who has the primary copy of the file and is responsible for creating the replicas. This owner is calculated by hashing the filename.
2) replicas : A owner would ask these nodes to store the replicas for a file. These are generally the next 2 nodes of the owner in the ring.

Node Composition:
Each node in HyDFS runs two services:

Command receiver: this receives commands from cli and then call appropriate functions.

This has multiple funcitons which are called by command reciever based on the commands given by the user.
This Server would have gRPC functions for:
1) CreateFile : Get the coordinator details and send a request to the owner to create the file.
2) GetFile : 

CoordinatorServer:

FileServer:
1) Receive the CreateFile request and then create a FileMetaData object for the file and add it to FileSystem o.

- Performs actual file operations: receiving/sending data, writing to disk, appending content.

- Uses goroutines and queues for concurrency.

- Stores files in three directories: OwnedFiles/, ReplicatedFiles/, TempFiles/.

Control Plane Communication

HyDFS uses gRPC with Protocol Buffers for communication between CoordinatorServers across nodes. This approach provides strong typing, automatic code generation, built-in support for retries and streaming.

End-to-End Flow of a HyDFS Operation (Verbose Version)

This section describes the complete lifecycle of a file operation in HyDFS, focusing on the append operation as a representative example. The flow assumes no failures and aims to preserve per-client append ordering, eventual consistency, and read-my-writes guarantees.

Create Operation: create localfile.txt /foo/bar.txt

Step 1: Client-Side Coordination
- The client (e.g., node P2) initiates a create operation by issuing the command create localfile.txt /foo/bar.txt.
- P2’s CoordinatorServer computes the hash of the target filename and determines the owner node using the consistent hashing ring.
- Then it calls a "CreateFileRequest" to the owner node’s FileServer (e.g., P4).

Step 2: Owner Node Processing
- P4 receives this call and creates a FileMetaData object and assigns this operation Id equal to 1 as the file is being created.
- Then it sends the response back to the client that it is ready to recieve this file. It also sends the FileMetaData object in the response.
- The client on receiving this object sends the actual file along with the FileMetaData object received through the CreateFile function. 
- The owner now receives the file as well as the FileMetaData object. It now adds this object to the FileSystem map. After this it creates a request to create replicas and add it to PendingOperations queue of FileSystem and return success response to the client.
- Client receives this and mark this as completed.

Step 3: Replication Setup
- There is a separate thread running to pick the Operations in the PendingOperations queue. 
- This would pick the request and determine the replica set (e.g. P5, P6). Then call the CreateReplicaRequest to these nodes.
- The nodes will return with the FileMetaData object which is of type Replica. Then it would call the CreateReplica to send this object with the file.
- The replicas would save the file and add the object to the FileSystem.

Append Operation: append localfile.txt /foo/bar.txt

Step 1: Client-Side Coordination
- The client (e.g., node P2) initiates a append operation by issuing the command append localfile.txt /foo/bar.txt.
- P2’s CoordinatorServer computes the hash of the target filename and determines the owner node using the consistent hashing ring.
- Then it calls a AppendFileRequest to the owner node’s FileServer (e.g., P4).

Step 2: Owner Node Processing
- P4 receives this call and gets the FileMetaData object for the respective file, then it gets the LastOperationId, increment it.
- It sends this id in the response back to the client.
- The client on receiving this operationId sends the actual file along with it through the AppendFile function. 
- The owner now receives the file as well as the operationId. It saves this file for now to the temp location. It now fetches the FileMetaData object for the file to which data is to be appended and then add this append operation to PendingOperations of that FileMetaData object.
- After this step is done it would also create a append Replica request to the PendingOperations queue of FileSystem and return success response to the client.
- Client receives this and mark this as completed.

Step 3: Replication Setup
- There is a separate thread running to pick the Operations in the PendingOperations queue. 
- This would pick the request and determine the replica set (e.g. P5, P6). Then call the AppendReplicaRequest to these nodes with the operationId and the file.
- These nodes will save the file to the temp and add the request to the PendingOperations TreeSet of that FileMetaData object.

Step 4: convergence
- There would be a convergence thread running on the PendingOperations TreeSet of all the files and if there are any pending Operations for a file then complete them. In this case it would append the file and save it.

Correctness Guarantees:

- Per-client append ordering via opID.

- Eventual consistency through converger thread.

- Read-my-writes ensured by consistent metadata updates.

GetFromReplica Operation

The client specifies a VM address and filename. The CoordinatorServer sends a request directly to the specified replica’s FileServer. The replica streams the file back to the client. This is useful for debugging or verifying replication correctness.

Merge Operation

The client issues a merge command. The CoordinatorServer identifies all replicas of the file. It sends a merge request to each replica. Each replica reads its local copy and computes a hash. The coordinator compares hashes and selects the majority version. It then sends overwrite commands to out-of-sync replicas to update their files. This ensures all replicas have identical content and maintains eventual consistency.

Supported File Operations

Command

Description

create localfilename HyDFSfilename

Copies local file to HyDFS. Only first create succeeds. Replicates to n successors.

get HyDFSfilename localfilename

Fetches file from HyDFS and saves locally. Ensures read-my-writes.

append localfilename HyDFSfilename

Appends content to HyDFS file. Ensures per-client ordering and eventual consistency.

merge HyDFSfilename

Merges file versions across replicas. Ensures identical content.

ls HyDFSfilename

Lists all VMs storing the file and its fileID.

liststore

Lists all files stored at current node.

getfromreplica VMaddress HyDFSfilename localfilename

Fetches file from a specific replica.

list_mem_ids

Lists membership with node IDs sorted.

multiappend HyDFSfilename VMi … VMj localfilei … localfilej

Launches concurrent appends from multiple VMs.

File Storage Layout

- OwnedFiles/: Files owned by this node

- ReplicatedFiles/: Files replicated from other nodes

- TempFiles/: Buffer files before appending

Go Data Structures

MembershipTable:

type MembershipTable struct {
    NodeID       string // IP + Port
    State        string
    Incarnation  int
    Timestamp    time.Time
}

HashSystem:

type HashSystem struct {
    func ComputeLocation(filename string) string
}

FileSystem{
	Files FileMetaData // A map of file related Metadata
			// key : FileName
			// Value : FIleMetaData object
    PendingOperations Queue // This can store operations that are to be executed like CreateReplicas or AppendReplicas
}

FileMetaData:

type FileMetaData struct {
    FileName         string
    LastModified     time.Time
    Hash             string
    Location         string
    Type             FileType // Enum: Self, Replica1, Replica2
    LastOperationId  int
    Operations       []Operation
    PendingOperations TreeSet // Sorted by opID
}

func (fm *FileMetaData) ComputeHash(file File) string {
    // Use existing library to compute hash
}

FileRequest:

type FileRequest struct {
    OperationType   OperationType // Enum: Create, Append, Get
    FileName        string
    FileLoc         string
    FileOperationID int
}

FileHandlers:

type FileHandlers struct {
    NoOfThreads   int
    RequestQueue  chan FileRequest
}

func (fh *FileHandlers) SendFile(req FileRequest) {
    // Launch goroutine to send file
}

func (fh *FileHandlers) ReceiveFile(req FileRequest) {
    // Launch goroutine to receive file
}

func (fh *FileHandlers) ConvergeFile() {
    // Background thread to apply pending operations
}

MainServerThread:

type MainServerThread struct {}

func (mst *MainServerThread) RequestListener() {
    // Listen for incoming requests
}

func (mst *MainServerThread) CreateFile(req FileRequest) {
    // Owner node handles creation
}

func (mst *MainServerThread) ReplicateFile(req FileRequest) {
    // Send to successors
}

func (mst *mst) MergeFile(filename string) {
    // Trigger merge across replicas
}

Design Highlights

- Decentralized Ownership: Determined via consistent hashing.

- Per-client Append Ordering: Maintained via opID.

- Asynchronous Data Flow: Goroutines for non-blocking replication.

- Eventually Consistent: Merge and background converger ensure convergence.

- Scalable: Designed to run on 10+ nodes with minimal coordination.

Go to
Page